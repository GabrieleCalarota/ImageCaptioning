{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GabrieleCalarota/ImageCaptioning/blob/master/Flickr30k_with_MS_COCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7raIOpwYiveh",
    "outputId": "5b72aa74-b624-4658-e971-aafe193a2a69"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pillow\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install -q tqdm\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install pydot\n",
    "!{sys.executable} -m pip install pydot-ng\n",
    "!{sys.executable} -m pip install graphviz\n",
    "!{sys.executable} -m pip install pydotplus\n",
    "!{sys.executable} -m pip install knockknock\n",
    "!{sys.executable} -m pip install keyring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpQCepLgi37m"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from knockknock import telegram_sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "cXiEjUk8i6Lh",
    "outputId": "e10c3a36-cd1e-4631-b501-8bc1e0103862"
   },
   "outputs": [],
   "source": [
    "# Download image files\n",
    "image_folder = '/flickr30k_images/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('dataset.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'https://casacalarota.onthewifi.com/wp-content/uploads/2020/10/archive.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2IC8_t3jMhu"
   },
   "outputs": [],
   "source": [
    "# extract result.csv\n",
    "result = pd.read_csv(PATH+'results.csv', sep=\"|\", error_bad_lines=False)\n",
    "result = result.replace({np.nan: None})\n",
    "result.columns\n",
    "annotations = result.groupby('image_name')[' comment'].apply(list).to_dict()\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckibAzcwjIcE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the json file\n",
    "# with open(annotation_file, 'r') as f:\n",
    "#    annotations = json.load(f)\n",
    "\n",
    "# Store captions and image names in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for filename in annotations:\n",
    "    caption_list = annotations[filename]\n",
    "    for caption in caption_list:\n",
    "      if isinstance(caption,str):\n",
    "        caption_parsed = '<start>' + caption + ' <end>'\n",
    "        all_captions.append(caption_parsed)\n",
    "        image_id = filename\n",
    "        full_coco_image_path = PATH + 'flickr30k_images/' + image_id\n",
    "\n",
    "        all_img_name_vector.append(full_coco_image_path)\n",
    "    \n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "# Set a random state\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "img_name_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "eVTQzkKD2YnR",
    "outputId": "5d127cb2-03b5-4ee6-acf2-074ce1f92949"
   },
   "outputs": [],
   "source": [
    "len(train_captions), len(img_name_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "SP512SAP2YnV",
    "outputId": "26a81047-d848-4f8e-a08f-90802e977739"
   },
   "outputs": [],
   "source": [
    "# inizializzo un'istanza del modelo InceptionV3 trainato su imagenet (classificazione immagini)\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "     \n",
    "# l'input del mio modello è uguale all'input di InceptionV 3                                            \n",
    "new_input = image_model.input\n",
    "# l'output del mio modelllo è uguale all'ultimo layer di InceptionV3, una convnet con attenzione\n",
    "hidden_layer=image_model.layers[-1].output\n",
    "\n",
    "# creo il mio modello\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBRM1UCv2YnZ"
   },
   "outputs": [],
   "source": [
    "#funzione per il preprocessing delle immagini in modo che siano coerenti con\n",
    "#l'input di InceptionV3\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.adjust_jpeg_quality(img, jpeg_quality=10)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "#funzione per mostrare il preprocessing fatto su un'immagine e quella originale\n",
    "def visualize(im_path, imAgmented, operation):\n",
    "    temp_image = np.array(Image.open(im_path))\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(temp_image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(operation)\n",
    "    plt.imshow(imAgmented)\n",
    "\n",
    "#cropped, path = load_image(\"E:\\\\Python\\\\Img_Caption\\\\COCO\\\\train2014\\\\COCO_train2014_000000000009.jpg\")\n",
    "# print(path)\n",
    "# Image.open(path)\n",
    "#visualize(path, cropped, \"cropped 90%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "VwGD7P6T2Ync",
    "outputId": "3c1fe073-1ae1-4c48-efff-e44bd8895c61",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "# creo un set di immagini sortate e uniche\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "# creo il dataset di immagini\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "\n",
    "# applico la funzione di preprocessing su tutte le immagini del dataset\n",
    "# tf.data.experimental.AUTOTUNE serve per fare la map in paraellelo in base alle\n",
    "# possibilità della macchina. Alla fine vengono suddivise le immagini in batch da 16\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "  #estraggo le features dai batch delle immagini dandole in pasto al nostro modello\n",
    "  batch_features = image_features_extract_model(img) # shape di sta roba -> 16x8x8x2048\n",
    "  #faccio reshape in modo che le features abbiano shape = 16x64x2048\n",
    "  #la funzione tf.reshape prende in input il tensore delle features (16x8x8x2048)\n",
    "  #e lo reshapa passando [16,-1,2048], dove il -1 è una wildcard che fa in modo che al suo\n",
    "  #posto ci vada una dimensione tale che il tensore finale sia compatibile con quello iniziale\n",
    "  #in questo caso dropa 8x8 e ci butta 64, in pratica -1 droppa dimensioni e le flatta  \n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              [batch_features.shape[0], -1, batch_features.shape[3]])\n",
    "\n",
    "  #salva il tutto su disco perché non ci starebbe in ram\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bz7skPBM2Ynf"
   },
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TowAKTg32Ynj"
   },
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "# preprocessing standard, limita le parole alle 10k più fequenti,\n",
    "# sostituisce le parole non conosciute (quindi dalla 10k+1 in poi) con\n",
    "# il singoletto \"<unk>\" e sassa tutti i caratteri speciali vari\n",
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "# fitta il tokenizer che abbiamo creato nella riga precedente con il set\n",
    "# di caption\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "# sostituisce le parole con gli int token corrispondenti\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqZiNAPS2Ynm"
   },
   "outputs": [],
   "source": [
    "# forza l'indice 0 del tokenizer ad essere il singoletto \"<pad>\"\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8bduOkx2Ynp"
   },
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "# fa di nuovo sta cosa (per il pad suppongo)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ok7hLnhb2Yns"
   },
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "\n",
    "# padda tutto senza lunghezza fissata (prende la più lunga). paddando mette degli 0\n",
    "# ma gli zeri abbiamo visto che sono <pad>\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i9G4fEW2Ynv"
   },
   "outputs": [],
   "source": [
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUUCtBoR2Yny"
   },
   "outputs": [],
   "source": [
    "# Create training and validation sets using an 80-20 split\n",
    "# classico split, ritorna 4 valori (due di test e due di train ovviamente)\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzTruB4t2Yn1"
   },
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfruWTmp2Yn4"
   },
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XV54DmxQ2Yn8"
   },
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "# carico le features che avevo salvato su disco\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQKoMwsj2YoB"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "# per uno shuffle perfetto dovremmo usare un BUFFER_SIZE più grande del dataset\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# prefetch serve per paralellizzare il processo del dataset\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpK-Z6Tc2YoE"
   },
   "outputs": [],
   "source": [
    "# subclassare Model è uno dei due modi di tf per creare un Model\n",
    "# usando questo metodo è necessario definire il layer in __init__\n",
    "# e implementare un metodo \"call\" che definisca il model's forward pass\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__() #standard\n",
    "    \n",
    "    #units è globale ed è 512. Questo è un normale layer Dense\n",
    "    #cioè un layer fully connected. L'activation è di default è linear (quindi non c'è)\n",
    "    #ci possiamo giocare volendo per vedere se migliora il tutto\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    \n",
    "    #units è globale ed è 512. Questo è un normale layer Dense\n",
    "    #cioè un layer fully connected. L'activation è di default è linear (quindi non c'è)\n",
    "    #ci possiamo giocare volendo per vedere se migliora il tutto\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    \n",
    "    #classico layer Dense di output, in modo che ci sia un solo output\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRS3wudY2YoH"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIk62XSh2YoJ"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EwECdmr2YoM"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL60rlO12YoP"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z21JQpp-2YoT"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/FLICKR30k\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il0GN_kc2YoW"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTZ5XywX2YoZ"
   },
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paw32vCi2Yob"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ISVGAA02Yoe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "CHAT_ID: int = \"***REMOVED***\"\n",
    "@telegram_sender(token=\"***REMOVED***\", chat_id=CHAT_ID)\n",
    "def training():\n",
    "  for epoch in range(start_epoch, EPOCHS):\n",
    "      start = time.time()\n",
    "      total_loss = 0\n",
    "\n",
    "      for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "          batch_loss, t_loss = train_step(img_tensor, target)\n",
    "          total_loss += t_loss\n",
    "\n",
    "          if batch % 100 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "      # storing the epoch end loss value to plot later\n",
    "      loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "      ckpt_manager.save()\n",
    "\n",
    "      print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                          total_loss/num_steps))\n",
    "      print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihB3K9Dz2Yog"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0pGXXX72Yok"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCOx-h3L2Yon"
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgLflEq82Yop"
   },
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "print(rid)\n",
    "image = img_name_val[rid]\n",
    "print(image)\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)\n",
    "\n",
    "Image.open(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JW-mqjF2Yor"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# TESTING PURPOSES\n",
    "from pathlib import Path\n",
    "curDir = os.getcwd()\n",
    "Path(curDir + \"/test/\").mkdir(parents=True, exist_ok=True)\n",
    "image_path = curDir+'/test/image.jpg'\n",
    "image_url = \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/instagram-captions-for-couples-1578955100.jpg\"\n",
    "with open(image_path, 'wb') as f:\n",
    "    f.write(requests.get(image_url).content)\n",
    "\n",
    "\n",
    "\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "curDir = os.getcwd()\n",
    "folder = \"FLICKR30K\"\n",
    "Path(curDir + f\"/model/{folder}/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(curDir + f\"/tokenizer/{folder}/\").mkdir(parents=True, exist_ok=True)\n",
    "encoder.save_weights(f\"model/{folder}/encoder\", save_format=\"tf\")\n",
    "decoder.save_weights(f\"model/{folder}/decoder\", save_format=\"tf\")\n",
    "with open(f'tokenizer/{folder}/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCigLcmH5EtP1tyP1z7MkQ",
   "include_colab_link": true,
   "name": "Flickr30k with MS_COCO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

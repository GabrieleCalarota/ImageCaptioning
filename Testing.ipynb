{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pillow\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install -q tqdm\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install pydot\n",
    "!{sys.executable} -m pip install pydot-ng\n",
    "!{sys.executable} -m pip install graphviz\n",
    "!{sys.executable} -m pip install pydotplus\n",
    "!{sys.executable} -m pip install knockknock\n",
    "!{sys.executable} -m pip install keyring\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from knockknock import telegram_sender\n",
    "\n",
    "# Currently the ‘memory growth’ option should be the same for all GPUs.\n",
    "# You should set the ‘memory growth’ option before initializing GPUs.\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image files\n",
    "image_folder_flickr = '/flickr30k_images/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder_flickr):\n",
    "  image_zip = tf.keras.utils.get_file('dataset.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'https://casacalarota.onthewifi.com/wp-content/uploads/2020/10/archive.zip',\n",
    "                                      extract = True)\n",
    "  PATH_flickr = os.path.dirname(image_zip) + image_folder_flickr\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH_flickr = os.path.abspath('.') + image_folder_flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract result.csv\n",
    "result = pd.read_csv(PATH_flickr+'results.csv', sep=\"|\", error_bad_lines=False)\n",
    "result = result.replace({np.nan: None})\n",
    "result.columns\n",
    "annotations_flickr = result.groupby('image_name')[' comment'].apply(list).to_dict()\n",
    "annotations_flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "annotation_folder_coco = '/annotations/'\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder_coco):\n",
    "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('.'),\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = True)\n",
    "  # annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "  os.remove(annotation_zip)\n",
    "annotation_file_coco = os.path.abspath('.')+'/annotations/captions_train2014.json'\n",
    "\n",
    "# Download image files\n",
    "image_folder_coco = '/train2014/'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder_coco):\n",
    "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH_coco = os.path.dirname(image_zip) + image_folder_coco\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH_coco = os.path.abspath('.') + image_folder_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the json file\n",
    "with open(annotation_file_coco, 'r') as f:\n",
    "    annotations_coco = json.load(f)\n",
    "\n",
    "# Store captions and image names in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for annot in annotations_coco['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH_coco + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "    \n",
    "\n",
    "# Read the json file\n",
    "# with open(annotation_file, 'r') as f:\n",
    "#    annotations = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "for filename in annotations_flickr:\n",
    "    caption_list = annotations_flickr[filename]\n",
    "    for caption in caption_list:\n",
    "      if isinstance(caption,str):\n",
    "        caption_parsed = '<start>' + caption + ' <end>'\n",
    "        all_captions.append(caption_parsed)\n",
    "        image_id = filename\n",
    "        full_coco_image_path = PATH_flickr + 'flickr30k_images/' + image_id\n",
    "\n",
    "        all_img_name_vector.append(full_coco_image_path)\n",
    "    \n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "# Set a random state\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "    \n",
    "\n",
    "# Shuffle captions and image_names together\n",
    "# Set a random state\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "threshold = 400000\n",
    "# seleziono solo 30k captions (circa 6k immagini)\n",
    "train_captions = train_captions[:threshold]\n",
    "img_name_vector = img_name_vector[:threshold]\n",
    "img_name_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top 5000 words from the vocabulary\n",
    "# preprocessing standard, limita le parole alle 10k più fequenti,\n",
    "# sostituisce le parole non conosciute (quindi dalla 10k+1 in poi) con\n",
    "# il singoletto \"<unk>\" e sassa tutti i caratteri speciali vari\n",
    "top_k = 10000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "# fitta il tokenizer che abbiamo creato nella riga precedente con il set\n",
    "# di caption\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "# sostituisce le parole con gli int token corrispondenti\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forza l'indice 0 del tokenizer ad essere il singoletto \"<pad>\"\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "# fa di nuovo sta cosa (per il pad suppongo)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "\n",
    "# padda tutto senza lunghezza fissata (prende la più lunga). paddando mette degli 0\n",
    "# ma gli zeri abbiamo visto che sono <pad>\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets using an 80-20 split\n",
    "# classico split, ritorna 4 valori (due di test e due di train ovviamente)\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "top_k = 10000\n",
    "vocab_size = top_k + 1\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "\n",
    "max_length=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"MS_COCO_FLICKR30\"\n",
    "from pathlib import Path\n",
    "curDir = os.getcwd()\n",
    "Path(curDir + f\"/model/{foldername}/\").mkdir(parents=True, exist_ok=True)\n",
    "Path(curDir + f\"/tokenizer/{foldername}/\").mkdir(parents=True, exist_ok=True)\n",
    "tokenizer_file_name = f\"tokenizer/{foldername}/tokenizer.pickle\"\n",
    "encoder_file_name = f\"model/{foldername}/encoder\"\n",
    "decoder_file_name = f\"model/{foldername}/decoder\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "from pickle import load\n",
    "tokenizer = load(open(tokenizer_file_name,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclassare Model è uno dei due modi di tf per creare un Model\n",
    "# usando questo metodo è necessario definire il layer in __init__\n",
    "# e implementare un metodo \"call\" che definisca il model's forward pass\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__() #standard\n",
    "    \n",
    "    #units è globale ed è 512. Questo è un normale layer Dense\n",
    "    #cioè un layer fully connected. L'activation è di default è linear (quindi non c'è)\n",
    "    #ci possiamo giocare volendo per vedere se migliora il tutto\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    \n",
    "    #units è globale ed è 512. Questo è un normale layer Dense\n",
    "    #cioè un layer fully connected. L'activation è di default è linear (quindi non c'è)\n",
    "    #ci possiamo giocare volendo per vedere se migliora il tutto\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    \n",
    "    #classico layer Dense di output, in modo che ci sia un solo output\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # score shape == (batch_size, 64, hidden_size)\n",
    "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    # you get 1 at the last axis because you are applying score to self.V\n",
    "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "encoder.load_weights(encoder_file_name)\n",
    "decoder.load_weights(decoder_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inizializzo un'istanza del modelo InceptionV3 trainato su imagenet (classificazione immagini)\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "     \n",
    "# l'input del mio modello è uguale all'input di InceptionV 3                                            \n",
    "new_input = image_model.input\n",
    "# l'output del mio modelllo è uguale all'ultimo layer di InceptionV3, una convnet con attenzione\n",
    "hidden_layer=image_model.layers[-1].output\n",
    "\n",
    "# creo il mio modello\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per il preprocessing delle immagini in modo che siano coerenti con\n",
    "#l'input di InceptionV3\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.adjust_jpeg_quality(img, jpeg_quality=10)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "#funzione per mostrare il preprocessing fatto su un'immagine e quella originale\n",
    "def visualize(im_path, imAgmented, operation):\n",
    "    temp_image = np.array(Image.open(im_path))\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(temp_image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(operation)\n",
    "    plt.imshow(imAgmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def get_Xigram(caption, n):\n",
    "    return list(ngrams(caption.split(), n))\n",
    "\n",
    "\n",
    "def score_Xigram(real_caption, predicted_caption, ngram=1):\n",
    "    real_Xigrams = get_Xigram(real_caption, ngram)\n",
    "    predicted_Xigrams = get_Xigram(predicted_caption, ngram)\n",
    "    total_number_hypothesis = len(real_Xigrams)\n",
    "    return sum([predicted_Xigrams.count(Xigram) for Xigram in real_Xigrams]) / total_number_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_ID: int = \"\"\n",
    "@telegram_sender(token=\"\", chat_id=CHAT_ID)\n",
    "def total_score():\n",
    "    sum_score_1 = 0\n",
    "    sum_score_2 = 0\n",
    "    sum_score_3 = 0\n",
    "    for index, image in enumerate(img_name_val):\n",
    "        # print(image)\n",
    "        real_caption =  ' '.join([tokenizer.index_word[i] for i in cap_val[index] if i not in [0]])\n",
    "        result, attention_plot = evaluate(image)\n",
    "        predicted_caption = '<start> ' + ' '.join(result)\n",
    "        # print(predicted_caption)\n",
    "        # print(real_caption)\n",
    "        # print(score_single_image)\n",
    "        sum_score_1 += score_Xigram(real_caption, predicted_caption, ngram=1)\n",
    "        sum_score_2 += score_Xigram(real_caption, predicted_caption, ngram=2)\n",
    "        sum_score_3 += score_Xigram(real_caption, predicted_caption, ngram=3)\n",
    "        if index % 100 == 0:\n",
    "            print(\"Img: \", image)\n",
    "            print ('Real Caption:', real_caption)\n",
    "            print ('Prediction Caption:', predicted_caption)\n",
    "            plot_attention(image, result, attention_plot)\n",
    "            perc_completed = (index + 1) / len(img_name_val) * 100\n",
    "            print(\"Percent test_set completed={perc_completed}\".format(perc_completed=perc_completed))\n",
    "    len_img_val = len(img_name_val)\n",
    "    return sum_score_1 / len_img_val, sum_score_2 / len_img_val, sum_score_3 / len_img_val\n",
    "\n",
    "unigram, bigram, trigram = total_score()\n",
    "print(\"UNIGRAM TOTAL SCORE: \", unigram)\n",
    "print(\"BIGRAM TOTAL SCORE: \", bigram)\n",
    "print(\"TRIGRAM TOTAL SCORE: \", trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "print(rid)\n",
    "image = img_name_val[rid]\n",
    "print(image)\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)\n",
    "\n",
    "Image.open(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# TESTING PURPOSES\n",
    "from pathlib import Path\n",
    "curDir = os.getcwd()\n",
    "Path(curDir + \"/test/\").mkdir(parents=True, exist_ok=True)\n",
    "image_path = curDir+'/test/image.jpg'\n",
    "image_url = \"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/instagram-captions-for-couples-1578955100.jpg\"\n",
    "with open(image_path, 'wb') as f:\n",
    "    f.write(requests.get(image_url).content)\n",
    "\n",
    "\n",
    "\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
